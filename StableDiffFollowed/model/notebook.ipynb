{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stable Diffusion Notebook\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Background of stable Diffusion***\n",
    "\n",
    "Powerful [open-source](https://github.com/Stability-AI/stablediffusion) model.\n",
    "\n",
    "Image-to-image, text-to-image, and inpainting (select part to remove)\n",
    "\n",
    "? Why does noise allow the model to learn better?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder\n",
    "\n",
    "We build our VAE, which compresses our image, so there is less necessary computation in our convolutional layers.\n",
    "\n",
    "After we run our latent vector through our unet, we put this through our decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of conv layer is: [5, 128, 255.5, 255.5]\n",
      "Shape of conv layer is: [5, 256, 127.5, 127.5]\n"
     ]
    }
   ],
   "source": [
    "''' Understanding How convolutional layers will affect shape'''\n",
    "\n",
    "# input is: (Batch, Channels, H,W)\n",
    "def get_shape(input,out_channels,kernel_size,padding,stride):\n",
    "    \n",
    "    h_out = ((input[2] + 2*padding - kernel_size) / stride) + 1\n",
    "    w_out = ((input[3] + 2*padding - kernel_size) / stride) + 1\n",
    "    \n",
    "    output = [input[0],out_channels,h_out,w_out]\n",
    "    \n",
    "    print(f\"Shape of conv layer is: {output}\")\n",
    "\n",
    "\n",
    "# should 128,128\n",
    "get_shape([5,3,512,512],out_channels=128,kernel_size=3,padding=0,stride=2)\n",
    "\n",
    "# should 256,256\n",
    "get_shape([5,128,256,256],out_channels=256,kernel_size=3,padding=0,stride=2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did they come up with the architecture?\n",
    "\n",
    "It must have already worked well in practice for another architecture. They have simply borrowed from other architectures,\n",
    "- You must be aware of what is working at the cutting edge in neural network architectures!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GroupNorm\n",
    "\n",
    "What is group normalization?\n",
    "\n",
    "Since our output has come from convolutions, we want 'groups' that are normalized in '*square-blocks*. So, each distinct group will be local features, rather than entire rows or collumns (batch or output).\n",
    "\n",
    "So, we are ***making closer features have the same distribution***. This effectively makes it so that: individual similar features are expressed relatively to other nearby features. For example, elements with in one half of the image will be expressed relative to eachother, and not the other half, because the other half is less related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3066,  1.0215,  2.2728, -0.5375],\n",
      "        [-0.7986,  0.2427,  1.1274, -0.2242],\n",
      "        [ 0.8298, -1.5215, -0.1798,  0.2187],\n",
      "        [-0.4769, -0.3294, -1.2319,  0.8011]])\n",
      "tensor([[ 0.3066,  0.3066,  1.0215,  1.0215,  2.2728,  2.2728, -0.5375, -0.5375],\n",
      "        [ 0.3066,  0.3066,  1.0215,  1.0215,  2.2728,  2.2728, -0.5375, -0.5375],\n",
      "        [-0.7986, -0.7986,  0.2427,  0.2427,  1.1274,  1.1274, -0.2242, -0.2242],\n",
      "        [-0.7986, -0.7986,  0.2427,  0.2427,  1.1274,  1.1274, -0.2242, -0.2242],\n",
      "        [ 0.8298,  0.8298, -1.5215, -1.5215, -0.1798, -0.1798,  0.2187,  0.2187],\n",
      "        [ 0.8298,  0.8298, -1.5215, -1.5215, -0.1798, -0.1798,  0.2187,  0.2187],\n",
      "        [-0.4769, -0.4769, -0.3294, -0.3294, -1.2319, -1.2319,  0.8011,  0.8011],\n",
      "        [-0.4769, -0.4769, -0.3294, -0.3294, -1.2319, -1.2319,  0.8011,  0.8011]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "# playing with up-sampling\n",
    "# batch,channels,height,width\n",
    "test = torch.randn((3,3,4,4))\n",
    "print(test[0,0]) # 1st channel, 1st batch\n",
    "\n",
    "# this will grow by just multiplying the pixel!\n",
    "ups = nn.Upsample(scale_factor=2)\n",
    "test = ups(test)\n",
    "print(test[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4814)\n"
     ]
    }
   ],
   "source": [
    "# testing clip loss w/ minimal implementation\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# image vectors B, Embd\n",
    "txt = torch.randn((12,100))\n",
    "images = torch.randn((12,100))\n",
    "\n",
    "# normalizing so add to 1\n",
    "txt = F.normalize(txt) \n",
    "images = F.normalize(images)\n",
    "\n",
    "t = 0.01 # learned temperature parameter for sensitivity scaling\n",
    "\n",
    "# get inner-products\n",
    "corr_logits = images@txt.T # : dot product of text and images, row=text,col=img\n",
    "\n",
    "# cross-entropy\n",
    "labels = torch.arange(corr_logits.shape[0])\n",
    "\n",
    "# for each row, picking out target label (collumn of row)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(corr_logits,labels)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Full implementation of CLIP\n",
    "\n",
    "We not be focusing on the Encoder/Decoder for the image, and the text,\n",
    "and we will abstract these away, downloading pre-trained models.\n",
    "\n",
    "We will focus on the actual CLIP loss-objective, and add a final linear layer\n",
    "to project our outputs from our expressive encoder\n",
    "\n",
    "'''\n",
    "\n",
    "# downloading our tokenizer\n",
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 768])\n"
     ]
    }
   ],
   "source": [
    "text = \"io kl ioioio\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "\n",
    "\n",
    "# now, we have a way to generate tokens from our transformer!\n",
    "print(output.last_hidden_state.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "manimtest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
