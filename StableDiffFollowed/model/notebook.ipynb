{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stable Diffusion Notebook\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Background of stable Diffusion***\n",
    "\n",
    "Powerful [open-source](https://github.com/Stability-AI/stablediffusion) model.\n",
    "\n",
    "Image-to-image, text-to-image, and inpainting (select part to remove)\n",
    "\n",
    "? Why does noise allow the model to learn better?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder\n",
    "\n",
    "We build our VAE, which compresses our image, so there is less necessary computation in our convolutional layers.\n",
    "\n",
    "After we run our latent vector through our unet, we put this through our decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of conv layer is: [5, 128, 255.5, 255.5]\n",
      "Shape of conv layer is: [5, 256, 127.5, 127.5]\n"
     ]
    }
   ],
   "source": [
    "''' Understanding How convolutional layers will affect shape'''\n",
    "\n",
    "# input is: (Batch, Channels, H,W)\n",
    "def get_shape(input,out_channels,kernel_size,padding,stride):\n",
    "    \n",
    "    h_out = ((input[2] + 2*padding - kernel_size) / stride) + 1\n",
    "    w_out = ((input[3] + 2*padding - kernel_size) / stride) + 1\n",
    "    \n",
    "    output = [input[0],out_channels,h_out,w_out]\n",
    "    \n",
    "    print(f\"Shape of conv layer is: {output}\")\n",
    "\n",
    "\n",
    "# should 128,128\n",
    "get_shape([5,3,512,512],out_channels=128,kernel_size=3,padding=0,stride=2)\n",
    "\n",
    "# should 256,256\n",
    "get_shape([5,128,256,256],out_channels=256,kernel_size=3,padding=0,stride=2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did they come up with the architecture?\n",
    "\n",
    "It must have already worked well in practice for another architecture. They have simply borrowed from other architectures,\n",
    "- You must be aware of what is working at the cutting edge in neural network architectures!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GroupNorm\n",
    "\n",
    "What is group normalization?\n",
    "\n",
    "Since our output has come from convolutions, we want 'groups' that are normalized in '*square-blocks*. So, each distinct group will be local features, rather than entire rows or collumns (batch or output).\n",
    "\n",
    "So, we are ***making closer features have the same distribution***. This effectively makes it so that: individual similar features are expressed relatively to other nearby features. For example, elements with in one half of the image will be expressed relative to eachother, and not the other half, because the other half is less related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6566,  0.2413, -1.9425, -0.1324],\n",
      "        [ 0.2438,  0.5191, -1.3799,  0.9107],\n",
      "        [-0.6993,  0.7703, -1.5209, -1.1541],\n",
      "        [-1.4260,  0.9945, -0.2328,  1.4681]])\n",
      "tensor([[-0.6566, -0.6566,  0.2413,  0.2413, -1.9425, -1.9425, -0.1324, -0.1324],\n",
      "        [-0.6566, -0.6566,  0.2413,  0.2413, -1.9425, -1.9425, -0.1324, -0.1324],\n",
      "        [ 0.2438,  0.2438,  0.5191,  0.5191, -1.3799, -1.3799,  0.9107,  0.9107],\n",
      "        [ 0.2438,  0.2438,  0.5191,  0.5191, -1.3799, -1.3799,  0.9107,  0.9107],\n",
      "        [-0.6993, -0.6993,  0.7703,  0.7703, -1.5209, -1.5209, -1.1541, -1.1541],\n",
      "        [-0.6993, -0.6993,  0.7703,  0.7703, -1.5209, -1.5209, -1.1541, -1.1541],\n",
      "        [-1.4260, -1.4260,  0.9945,  0.9945, -0.2328, -0.2328,  1.4681,  1.4681],\n",
      "        [-1.4260, -1.4260,  0.9945,  0.9945, -0.2328, -0.2328,  1.4681,  1.4681]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "# playing with up-sampling\n",
    "# batch,channels,height,width\n",
    "test = torch.randn((3,3,4,4))\n",
    "print(test[0,0]) # 1st channel, 1st batch\n",
    "\n",
    "# this will grow by just multiplying the pixel!\n",
    "ups = nn.Upsample(scale_factor=2)\n",
    "test = ups(test)\n",
    "print(test[0,0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "manimtest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
