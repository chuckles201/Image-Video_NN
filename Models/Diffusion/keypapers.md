# Key Diffusion Papers that defined the field


#### Links that helped me:
- [top 6 papers](https://www.topbots.com/research-papers-diffusion-models/)


---------------------------------------------------
## Top research papers

Going over most important research that led to our modern-day capability to generate amazing images.

### 0. First diffusion model
Introduction of mathematics behind diffusion.
Links:
- [paper](https://arxiv.org/pdf/1503.03585)


### 1. Denoising Diffusion probabilistic models
- Key paper that expanded on the original idea of diffusion to create a model with stunning ability to generate images with DDPMS (denoising diffusion probabilistic models)

- latent space is learned encoder that maps high-dimensional pixels to low-dimensional vector representation: this was used to better represent images, and images with similar vectors and similar meanings, with different parts of the vector corresponding to different meaning

Links:

- [site](https://hojonathanho.github.io/diffusion/)

- [code](https://github.com/hojonathanho/diffusion)


### 2. Diffusion Models Beat GANS (openAI)

- Used DDPM to beat 'GANS' (Generative Adversarial Networks) which were the traditionally dominant model in image generation
- Showed that DDPMs where better suited for the task because of
- It was also discovered that *one specific hyperparamter* controlled diversity of images vs. fidelity (how strictly normal the image is)

Links:
- [paper](https://arxiv.org/pdf/2105.05233)
- [explanation vid.](https://www.youtube.com/watch?v=W-O7AZNzbzQ&ab_channel=YannicKilcher)
- [code](https://github.com/openai/guided-diffusion)


### 3. Stable Diffusion

- Problem: diffusion models take up a lot of compute. The creators of this model decided to adress this and create a model that had fewer parameters. To do this, they used pre-trained autoencoders, which then fed into the model, reducing the number of parameters, and therefore the complexity of the model.

- they also introduced 'cross-attention'
- They combined a [UNET](unet) with transformer architecture

Links:
- [code](https://github.com/CompVis/latent-diffusion)
- [paper](https://arxiv.org/abs/2112.10752)


### 4. DALL-E 2 (Open AI)

- Improved upon OpenAIs original DALL-E model (text-to-image generation)
- Model has two steps:
    1. Embedding generation
    2. Devoder that takes eembedding to draw picture

This allows us to create abstract images or photorealistic, based on how we describe the embedding, which is sort of like to models 'idea' of an image.

Links:
- [paper](https://arxiv.org/pdf/2204.06125.pdf)
- [blogpost](https://openai.com/dall-e-2/)




### 5. Imagen by Google

- Found that increasing the size of the *language model* that encoded the representations for the diffusion model is actually better than scaling up the diffusion image model itself.

- Model:
    1.  Large language model (like BERT to understand meaning) used to understand the text input, weights are frozen
    2. Diffusion model maps the text embedding generated by pretrained LLM to 64x64 image
    3. Another 'super-resolution' diffusion model makes the image high-res.

Links:
- [post](https://imagen.research.google/)
- [paper](https://arxiv.org/pdf/2205.11487)
- [unofficial-code](https://github.com/lucidrains/imagen-pytorch)




### 6. ControlNet by Stanford Research team

- Framework designed to 'control' pretrained diffusion models to handle different types of input (IE: used trained diffusion model to recognize simple sketches and generate an embedding based on this)

Method:
    1. Copies weights from large diffusion model, and has a 'trainable' part of the network which new inputs can be learned, and a 'locked part' that does the input generation
    2. New Zero convolution layer

This paper allows now for large image diffusion models to have additional input to allow for more fine control over what what we want outputted! This also is very quick and does not require signifcantly more training.

Links:
 - [paper](https://arxiv.org/abs/2302.05543)
 - [code](https://github.com/lllyasviel/ControlNet)
 - [blog post](https://github.com/lllyasviel/ControlNet/discussions/188)



